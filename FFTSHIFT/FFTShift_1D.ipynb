{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FFTShift_1D.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wGzjN41wUkf",
        "outputId": "2e8fafa9-a7d1-4c1f-a2a6-b7df85b092c6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycuda in /usr/local/lib/python3.7/dist-packages (2022.1)\n",
            "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pycuda) (1.4.4)\n",
            "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.7/dist-packages (from pycuda) (2022.1.12)\n",
            "Requirement already satisfied: mako in /usr/local/lib/python3.7/dist-packages (from pycuda) (1.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.7/dist-packages (from pytools>=2011.2->pycuda) (4.1.1)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytools>=2011.2->pycuda) (2.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (2.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->mako->pycuda) (3.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pycuda.driver as drv\n",
        "import pycuda.autoinit\n",
        "drv.init()\n",
        "print(\"%d device(s) found.\" % drv.Device.count())\n",
        "for i in range(drv.Device.count()):\n",
        "  dev = drv.Device(i)\n",
        "  print(\"Device #%d: %s\" % (i, dev.name()))\n",
        "  print(\" Compute Capability: %d.%d\" % dev.compute_capability())\n",
        "  print(\" Total Memory: %s GB\" % (dev.total_memory() // (1024 * 1024 * 1024)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DQc_WvVxLDp",
        "outputId": "a7573c33-50ec-43ef-e03a-9aeb39aa0afb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 device(s) found.\n",
            "Device #0: Tesla T4\n",
            " Compute Capability: 7.5\n",
            " Total Memory: 14 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClEm0HVpGXTa",
        "outputId": "9ff1a304-c996-4b40-cae7-b1ffbc2c3a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgXb4r6OGxBm",
        "outputId": "eb14d2d9-5c8b-4f80-94b0-6f691c49e3ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-50gwddmr\n",
            "  Running command git clone -q https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-50gwddmr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWlMGQWav23J",
        "outputId": "859c1399-aed4-4a3f-aaf6-04f2e2b888bd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "directory /content/src already exists\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda --name TimingGPU.h\n",
        "\n",
        "#ifndef __TIMINGGPU_H__\n",
        "#define __TIMINGGPU_H__\n",
        "\n",
        "/**************/\n",
        "/* TIMING GPU */\n",
        "/**************/\n",
        "\n",
        "// Events are a part of CUDA API and provide a system independent way to measure execution times on CUDA devices with approximately 0.5\n",
        "// microsecond precision.\n",
        "\n",
        "struct PrivateTimingGPU;\n",
        "\n",
        "class TimingGPU\n",
        "{\n",
        "private:\n",
        "\tPrivateTimingGPU *privateTimingGPU;\n",
        "\n",
        "public:\n",
        "\n",
        "\tTimingGPU();\n",
        "\n",
        "\t~TimingGPU();\n",
        "\n",
        "\tvoid StartCounter();\n",
        "\tvoid StartCounterFlags();\n",
        "\n",
        "\tfloat GetCounter();\n",
        "\n",
        "}; // TimingCPU class\n",
        "\n",
        "#endif"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gL65hZGBm_cQ",
        "outputId": "e42dfdfe-1ce5-4293-b3eb-adb947742c4b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/TimingGPU.h'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda --name Utilities.h\n",
        "\n",
        "#ifndef UTILITIES_H\n",
        "#define UTILITIES_H\n",
        "\n",
        "#include <cusolverDn.h>\n",
        "#include <cublas_v2.h>\n",
        "#include <cusparse_v2.h>\n",
        "#include <curand.h>\n",
        "#include <cufft.h>\n",
        "\n",
        "//#include <thrust/pair.h>\n",
        "\n",
        "//extern \"C\" int iDivUp(int, int);\n",
        "__host__ __device__ int iDivUp(int, int);\n",
        "extern \"C\" void gpuErrchk(cudaError_t);\n",
        "extern \"C\" void cusolveSafeCall(cusolverStatus_t);\n",
        "extern \"C\" void cublasSafeCall(cublasStatus_t);\n",
        "extern \"C\" void cufftSafeCall(cufftResult err);\n",
        "extern \"C\" void cusparseSafeCall(cusparseStatus_t err);\n",
        "extern \"C\" void curandSafeCall(curandStatus_t err);\n",
        "\n",
        "template <class T>\n",
        "void reverseArray(const T * __restrict__, T * __restrict__, const int, const T a = static_cast<T>(1));\n",
        "\n",
        "//template <class T>\n",
        "//thrust::pair<T *,T *> Cartesian2Polar(const T * __restrict__ d_x, const T * __restrict__ d_y, const int N, const T a = static_cast<T>(1));\n",
        "//\n",
        "//template <class T>\n",
        "//thrust::pair<T *,T *> h_Cartesian2Polar(const T * __restrict__ d_x, const T * __restrict__ d_y, const int N, const T a = static_cast<T>(1));\n",
        "\n",
        "template<class T>\n",
        "T h_l2_norm(T *v1, T *v2, const int N);\n",
        "\n",
        "template <class T>\n",
        "void linearCombination(const T * __restrict__, const T * __restrict__, T * __restrict__, const int, const int, const cublasHandle_t);\n",
        "\n",
        "void linearCombination(const float * __restrict__, const float * __restrict__, float * __restrict__,\n",
        "\tconst int, const int, const cublasHandle_t);\n",
        "\n",
        "void linearCombination(const double * __restrict__, const double * __restrict__, double * __restrict__,\n",
        "\tconst int, const int, const cublasHandle_t);\n",
        "\n",
        "template<class T>\n",
        "void vectorAddConstant(T * __restrict__, const T, const int);\n",
        "\n",
        "template<class T>\n",
        "void vectorMulConstant(T * __restrict__, const T, const int);\n",
        "\n",
        "template<class T>\n",
        "void h_vectorMulConstant(T * __restrict__, const T, const int);\n",
        "\n",
        "template<class T>\n",
        "__host__ __device__ T fma2(T, T, T);\n",
        "\n",
        "__device__ int modulo(int, int);\n",
        "\n",
        "__device__ double atomicAdd(double *, double);\n",
        "__device__ float  atomicMin(float *, float);\n",
        "\n",
        "double deg2rad(double);\n",
        "\n",
        "void cudaMemoryUsage();\n",
        "\n",
        "/**************************/\n",
        "/* TEMPLATE SHARED MEMORY */\n",
        "/**************************/\n",
        "// --- Credit to the simpleTemplates CUDA sample\n",
        "template <typename T>\n",
        "struct SharedMemory\n",
        "{\n",
        "\t// Ensure that we won't compile any un-specialized types\n",
        "\t__device__ T *getPointer()\n",
        "\t{\n",
        "\t\textern __device__ void error(void);\n",
        "\t\terror();\n",
        "\t\treturn NULL;\n",
        "\t}\n",
        "};\n",
        "\n",
        "// Following are the specializations for the following types.\n",
        "// int, uint, char, uchar, short, ushort, long, ulong, bool, float, and double\n",
        "// One could also specialize it for user-defined types.\n",
        "\n",
        "template <>\n",
        "struct SharedMemory <int>\n",
        "{\n",
        "\t__device__ int *getPointer()\n",
        "\t{\n",
        "\t\textern __shared__ int s_int[];\n",
        "\t\treturn s_int;\n",
        "\t}\n",
        "};\n",
        "\n",
        "template <>\n",
        "struct SharedMemory <unsigned int>\n",
        "{\n",
        "\t__device__ unsigned int *getPointer()\n",
        "\t{\n",
        "\t\textern __shared__ unsigned int s_uint[];\n",
        "\t\treturn s_uint;\n",
        "\t}\n",
        "};\n",
        "\n",
        "template <>\n",
        "struct SharedMemory <char>\n",
        "{\n",
        "\t__device__ char *getPointer()\n",
        "\t{\n",
        "\t\textern __shared__ char s_char[];\n",
        "\t\treturn s_char;\n",
        "\t}\n",
        "};\n",
        "\n",
        "template <>\n",
        "struct SharedMemory <unsigned char>\n",
        "{\n",
        "\t__device__ unsigned char *getPointer()\n",
        "\t{\n",
        "\t\textern __shared__ unsigned char s_uchar[];\n",
        "\t\treturn s_uchar;\n",
        "\t}\n",
        "};\n",
        "\n",
        "template <>\n",
        "struct SharedMemory <short>\n",
        "{\n",
        "\t__device__ short *getPointer()\n",
        "\t{\n",
        "\t\textern __shared__ short s_short[];\n",
        "\t\treturn s_short;\n",
        "\t}\n",
        "};\n",
        "\n",
        "template <>\n",
        "struct SharedMemory <unsigned short>\n",
        "{\n",
        "\t__device__ unsigned short *getPointer()\n",
        "\t{\n",
        "\t\textern __shared__ unsigned short s_ushort[];\n",
        "\t\treturn s_ushort;\n",
        "\t}\n",
        "};\n",
        "\n",
        "template <>\n",
        "struct SharedMemory <long>\n",
        "{\n",
        "\t__device__ long *getPointer()\n",
        "\t{\n",
        "\t\textern __shared__ long s_long[];\n",
        "\t\treturn s_long;\n",
        "\t}\n",
        "};\n",
        "\n",
        "template <>\n",
        "struct SharedMemory <unsigned long>\n",
        "{\n",
        "\t__device__ unsigned long *getPointer()\n",
        "\t{\n",
        "\t\textern __shared__ unsigned long s_ulong[];\n",
        "\t\treturn s_ulong;\n",
        "\t}\n",
        "};\n",
        "\n",
        "template <>\n",
        "struct SharedMemory <bool>\n",
        "{\n",
        "\t__device__ bool *getPointer()\n",
        "\t{\n",
        "\t\textern __shared__ bool s_bool[];\n",
        "\t\treturn s_bool;\n",
        "\t}\n",
        "};\n",
        "\n",
        "template <>\n",
        "struct SharedMemory <float>\n",
        "{\n",
        "\t__device__ float *getPointer()\n",
        "\t{\n",
        "\t\textern __shared__ float s_float[];\n",
        "\t\treturn s_float;\n",
        "\t}\n",
        "};\n",
        "\n",
        "template <>\n",
        "struct SharedMemory <double>\n",
        "{\n",
        "\t__device__ double *getPointer()\n",
        "\t{\n",
        "\t\textern __shared__ double s_double[];\n",
        "\t\treturn s_double;\n",
        "\t}\n",
        "};\n",
        "\n",
        "#endif"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "31opfbIGny9F",
        "outputId": "bb9d2a17-c0a2-486e-f16f-f3b2a10972b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/Utilities.h'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda --name Utilities.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "//#include <math.h>\n",
        "\n",
        "#include \"cuda_runtime.h\"\n",
        "#include <cuda.h>\n",
        "\n",
        "#include <cusolverDn.h>\n",
        "#include <cublas_v2.h>\n",
        "#include <cufft.h>\n",
        "\n",
        "#include \"Utilities.h\"\n",
        "\n",
        "#define DEBUG\n",
        "\n",
        "#define PI_R         3.14159265358979323846f\n",
        "\n",
        "/*******************/\n",
        "/* iDivUp FUNCTION */\n",
        "/*******************/\n",
        "//extern \"C\" int iDivUp(int a, int b){ return ((a % b) != 0) ? (a / b + 1) : (a / b); }\n",
        "__host__ __device__ int iDivUp(int a, int b) { return ((a % b) != 0) ? (a / b + 1) : (a / b); }\n",
        "\n",
        "/********************/\n",
        "/* CUDA ERROR CHECK */\n",
        "/********************/\n",
        "// --- Credit to http://stackoverflow.com/questions/14038589/what-is-the-canonical-way-to-check-for-errors-using-the-cuda-runtime-api\n",
        "void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true)\n",
        "{\n",
        "\tif (code != cudaSuccess)\n",
        "\t{\n",
        "\t\tfprintf(stderr, \"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "\t\tif (abort) { exit(code); }\n",
        "\t}\n",
        "}\n",
        "\n",
        "extern \"C\" void gpuErrchk(cudaError_t ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "\n",
        "/**************************/\n",
        "/* CUSOLVE ERROR CHECKING */\n",
        "/**************************/\n",
        "static const char *_cusolverGetErrorEnum(cusolverStatus_t error)\n",
        "{\n",
        "\tswitch (error)\n",
        "\t{\n",
        "\tcase CUSOLVER_STATUS_SUCCESS:\n",
        "\t\treturn \"CUSOLVER_SUCCESS\";\n",
        "\n",
        "\tcase CUSOLVER_STATUS_NOT_INITIALIZED:\n",
        "\t\treturn \"CUSOLVER_STATUS_NOT_INITIALIZED\";\n",
        "\n",
        "\tcase CUSOLVER_STATUS_ALLOC_FAILED:\n",
        "\t\treturn \"CUSOLVER_STATUS_ALLOC_FAILED\";\n",
        "\n",
        "\tcase CUSOLVER_STATUS_INVALID_VALUE:\n",
        "\t\treturn \"CUSOLVER_STATUS_INVALID_VALUE\";\n",
        "\n",
        "\tcase CUSOLVER_STATUS_ARCH_MISMATCH:\n",
        "\t\treturn \"CUSOLVER_STATUS_ARCH_MISMATCH\";\n",
        "\n",
        "\tcase CUSOLVER_STATUS_EXECUTION_FAILED:\n",
        "\t\treturn \"CUSOLVER_STATUS_EXECUTION_FAILED\";\n",
        "\n",
        "\tcase CUSOLVER_STATUS_INTERNAL_ERROR:\n",
        "\t\treturn \"CUSOLVER_STATUS_INTERNAL_ERROR\";\n",
        "\n",
        "\tcase CUSOLVER_STATUS_MATRIX_TYPE_NOT_SUPPORTED:\n",
        "\t\treturn \"CUSOLVER_STATUS_MATRIX_TYPE_NOT_SUPPORTED\";\n",
        "\n",
        "\t}\n",
        "\n",
        "\treturn \"<unknown>\";\n",
        "}\n",
        "\n",
        "inline void __cusolveSafeCall(cusolverStatus_t err, const char *file, const int line)\n",
        "{\n",
        "\tif (CUSOLVER_STATUS_SUCCESS != err) {\n",
        "\t\tfprintf(stderr, \"CUSOLVE error in file '%s', line %d, error: %s \\nterminating!\\n\", __FILE__, __LINE__, \\\n",
        "\t\t\t_cusolverGetErrorEnum(err)); \\\n",
        "\t\t\tassert(0); \\\n",
        "\t}\n",
        "}\n",
        "\n",
        "extern \"C\" void cusolveSafeCall(cusolverStatus_t err) { __cusolveSafeCall(err, __FILE__, __LINE__); }\n",
        "\n",
        "/*************************/\n",
        "/* CUBLAS ERROR CHECKING */\n",
        "/*************************/\n",
        "static const char *_cublasGetErrorEnum(cublasStatus_t error)\n",
        "{\n",
        "\tswitch (error)\n",
        "\t{\n",
        "\tcase CUBLAS_STATUS_SUCCESS:\n",
        "\t\treturn \"CUBLAS_STATUS_SUCCESS\";\n",
        "\n",
        "\tcase CUBLAS_STATUS_NOT_INITIALIZED:\n",
        "\t\treturn \"CUBLAS_STATUS_NOT_INITIALIZED\";\n",
        "\n",
        "\tcase CUBLAS_STATUS_ALLOC_FAILED:\n",
        "\t\treturn \"CUBLAS_STATUS_ALLOC_FAILED\";\n",
        "\n",
        "\tcase CUBLAS_STATUS_INVALID_VALUE:\n",
        "\t\treturn \"CUBLAS_STATUS_INVALID_VALUE\";\n",
        "\n",
        "\tcase CUBLAS_STATUS_ARCH_MISMATCH:\n",
        "\t\treturn \"CUBLAS_STATUS_ARCH_MISMATCH\";\n",
        "\n",
        "\tcase CUBLAS_STATUS_MAPPING_ERROR:\n",
        "\t\treturn \"CUBLAS_STATUS_MAPPING_ERROR\";\n",
        "\n",
        "\tcase CUBLAS_STATUS_EXECUTION_FAILED:\n",
        "\t\treturn \"CUBLAS_STATUS_EXECUTION_FAILED\";\n",
        "\n",
        "\tcase CUBLAS_STATUS_INTERNAL_ERROR:\n",
        "\t\treturn \"CUBLAS_STATUS_INTERNAL_ERROR\";\n",
        "\n",
        "\tcase CUBLAS_STATUS_NOT_SUPPORTED:\n",
        "\t\treturn \"CUBLAS_STATUS_NOT_SUPPORTED\";\n",
        "\n",
        "\tcase CUBLAS_STATUS_LICENSE_ERROR:\n",
        "\t\treturn \"CUBLAS_STATUS_LICENSE_ERROR\";\n",
        "\t}\n",
        "\n",
        "\treturn \"<unknown>\";\n",
        "}\n",
        "\n",
        "inline void __cublasSafeCall(cublasStatus_t err, const char *file, const int line)\n",
        "{\n",
        "\tif (CUBLAS_STATUS_SUCCESS != err) {\n",
        "\t\tfprintf(stderr, \"CUBLAS error in file '%s', line %d, error: %s\\nterminating!\\n\", __FILE__, __LINE__, \\\n",
        "\t\t\t_cublasGetErrorEnum(err)); \\\n",
        "\t\t\tassert(0); \\\n",
        "\t}\n",
        "}\n",
        "\n",
        "extern \"C\" void cublasSafeCall(cublasStatus_t err) { __cublasSafeCall(err, __FILE__, __LINE__); }\n",
        "\n",
        "/************************/\n",
        "/* CUFFT ERROR CHECKING */\n",
        "/************************/\n",
        "// See http://stackoverflow.com/questions/16267149/cufft-error-handling\n",
        "static const char *_cudaGetErrorEnum(cufftResult error)\n",
        "{\n",
        "\tswitch (error)\n",
        "\t{\n",
        "\tcase CUFFT_SUCCESS:\n",
        "\t\treturn \"CUFFT_SUCCESS - The cuFFT operation was successful\";\n",
        "\n",
        "\tcase CUFFT_INVALID_PLAN:\n",
        "\t\treturn \"CUFFT_INVALID_PLAN - cuFFT was passed an invalid plan handle\";\n",
        "\n",
        "\tcase CUFFT_ALLOC_FAILED:\n",
        "\t\treturn \"CUFFT_ALLOC_FAILED - cuFFT failed to allocate GPU or CPU memory\";\n",
        "\n",
        "\tcase CUFFT_INVALID_TYPE:\n",
        "\t\treturn \"CUFFT_INVALID_TYPE - No longer used\";\n",
        "\n",
        "\tcase CUFFT_INVALID_VALUE:\n",
        "\t\treturn \"CUFFT_INVALID_VALUE - User specified an invalid pointer or parameter\";\n",
        "\n",
        "\tcase CUFFT_INTERNAL_ERROR:\n",
        "\t\treturn \"CUFFT_INTERNAL_ERROR - Driver or internal cuFFT library error\";\n",
        "\n",
        "\tcase CUFFT_EXEC_FAILED:\n",
        "\t\treturn \"CUFFT_EXEC_FAILED - Failed to execute an FFT on the GPU\";\n",
        "\n",
        "\tcase CUFFT_SETUP_FAILED:\n",
        "\t\treturn \"CUFFT_SETUP_FAILED - The cuFFT library failed to initialize\";\n",
        "\n",
        "\tcase CUFFT_INVALID_SIZE:\n",
        "\t\treturn \"CUFFT_INVALID_SIZE - User specified an invalid transform size\";\n",
        "\n",
        "\tcase CUFFT_UNALIGNED_DATA:\n",
        "\t\treturn \"CUFFT_UNALIGNED_DATA - No longer used\";\n",
        "\n",
        "\tcase CUFFT_INCOMPLETE_PARAMETER_LIST:\n",
        "\t\treturn \"CUFFT_INCOMPLETE_PARAMETER_LIST - Missing parameters in call\";\n",
        "\n",
        "\tcase CUFFT_INVALID_DEVICE:\n",
        "\t\treturn \"CUFFT_INVALID_DEVICE - Execution of a plan was on different GPU than plan creation\";\n",
        "\n",
        "\tcase CUFFT_PARSE_ERROR:\n",
        "\t\treturn \"CUFFT_PARSE_ERROR - Internal plan database error\";\n",
        "\n",
        "\tcase CUFFT_NO_WORKSPACE:\n",
        "\t\treturn \"CUFFT_NO_WORKSPACE - No workspace has been provided prior to plan execution\";\n",
        "\n",
        "\tcase CUFFT_NOT_IMPLEMENTED:\n",
        "\t\treturn \"CUFFT_NOT_IMPLEMENTED - Function does not implement functionality for parameters given\";\n",
        "\n",
        "\tcase CUFFT_LICENSE_ERROR:\n",
        "\t\treturn \"CUFFT_LICENSE_ERROR - Used in previous versions\";\n",
        "\n",
        "\tcase CUFFT_NOT_SUPPORTED:\n",
        "\t\treturn \"CUFFT_NOT_SUPPORTED - Operation is not supported for parameters given\";\n",
        "\t}\n",
        "\n",
        "\treturn \"<unknown>\";\n",
        "}\n",
        "\n",
        "// --- CUFFTSAFECALL\n",
        "inline void cufftAssert(cufftResult err, const char *file, const int line, bool abort = true)\n",
        "{\n",
        "\tif (CUFFT_SUCCESS != err) {\n",
        "\t\tfprintf(stderr, \"CUFFTassert: Error nr. %d - %s %s %d\\n\", err, _cudaGetErrorEnum(err), __FILE__, __LINE__);\n",
        "\t\tif (abort) exit(err);\n",
        "\t}\n",
        "}\n",
        "\n",
        "extern \"C\" void cufftSafeCall(cufftResult err) { cufftAssert(err, __FILE__, __LINE__); }\n",
        "\n",
        "/***************************/\n",
        "/* CUSPARSE ERROR CHECKING */\n",
        "/***************************/\n",
        "static const char *_cusparseGetErrorEnum(cusparseStatus_t error)\n",
        "{\n",
        "\tswitch (error)\n",
        "\t{\n",
        "\n",
        "\tcase CUSPARSE_STATUS_SUCCESS:\n",
        "\t\treturn \"CUSPARSE_STATUS_SUCCESS\";\n",
        "\n",
        "\tcase CUSPARSE_STATUS_NOT_INITIALIZED:\n",
        "\t\treturn \"CUSPARSE_STATUS_NOT_INITIALIZED\";\n",
        "\n",
        "\tcase CUSPARSE_STATUS_ALLOC_FAILED:\n",
        "\t\treturn \"CUSPARSE_STATUS_ALLOC_FAILED\";\n",
        "\n",
        "\tcase CUSPARSE_STATUS_INVALID_VALUE:\n",
        "\t\treturn \"CUSPARSE_STATUS_INVALID_VALUE\";\n",
        "\n",
        "\tcase CUSPARSE_STATUS_ARCH_MISMATCH:\n",
        "\t\treturn \"CUSPARSE_STATUS_ARCH_MISMATCH\";\n",
        "\n",
        "\tcase CUSPARSE_STATUS_MAPPING_ERROR:\n",
        "\t\treturn \"CUSPARSE_STATUS_MAPPING_ERROR\";\n",
        "\n",
        "\tcase CUSPARSE_STATUS_EXECUTION_FAILED:\n",
        "\t\treturn \"CUSPARSE_STATUS_EXECUTION_FAILED\";\n",
        "\n",
        "\tcase CUSPARSE_STATUS_INTERNAL_ERROR:\n",
        "\t\treturn \"CUSPARSE_STATUS_INTERNAL_ERROR\";\n",
        "\n",
        "\tcase CUSPARSE_STATUS_MATRIX_TYPE_NOT_SUPPORTED:\n",
        "\t\treturn \"CUSPARSE_STATUS_MATRIX_TYPE_NOT_SUPPORTED\";\n",
        "\n",
        "\tcase CUSPARSE_STATUS_ZERO_PIVOT:\n",
        "\t\treturn \"CUSPARSE_STATUS_ZERO_PIVOT\";\n",
        "\t}\n",
        "\n",
        "\treturn \"<unknown>\";\n",
        "}\n",
        "\n",
        "inline void __cusparseSafeCall(cusparseStatus_t err, const char *file, const int line)\n",
        "{\n",
        "\tif (CUSPARSE_STATUS_SUCCESS != err) {\n",
        "\t\tfprintf(stderr, \"CUSPARSE error in file '%s', line %d, error %s\\nterminating!\\n\", __FILE__, __LINE__, \\\n",
        "\t\t\t_cusparseGetErrorEnum(err)); \\\n",
        "\t\t\tassert(0); \\\n",
        "\t}\n",
        "}\n",
        "\n",
        "extern \"C\" void cusparseSafeCall(cusparseStatus_t err) { __cusparseSafeCall(err, __FILE__, __LINE__); }\n",
        "\n",
        "/*************************/\n",
        "/* CURAND ERROR CHECKING */\n",
        "/*************************/\n",
        "static const char *_curandGetErrorEnum(curandStatus_t error)\n",
        "{\n",
        "\tswitch (error)\n",
        "\t{\n",
        "\tcase CURAND_STATUS_SUCCESS:\n",
        "\t\treturn \"CURAND_SUCCESS\";\n",
        "\n",
        "\tcase CURAND_STATUS_VERSION_MISMATCH:\n",
        "\t\treturn \"CURAND_STATUS_VERSION_MISMATCH\";\n",
        "\n",
        "\tcase CURAND_STATUS_NOT_INITIALIZED:\n",
        "\t\treturn \"CURAND_STATUS_NOT_INITIALIZED\";\n",
        "\n",
        "\tcase CURAND_STATUS_ALLOCATION_FAILED:\n",
        "\t\treturn \"CURAND_STATUS_ALLOCATION_FAILED\";\n",
        "\n",
        "\tcase CURAND_STATUS_TYPE_ERROR:\n",
        "\t\treturn \"CURAND_STATUS_TYPE_ERROR\";\n",
        "\n",
        "\tcase CURAND_STATUS_OUT_OF_RANGE:\n",
        "\t\treturn \"CURAND_STATUS_OUT_OF_RANGE\";\n",
        "\n",
        "\tcase CURAND_STATUS_LENGTH_NOT_MULTIPLE:\n",
        "\t\treturn \"CURAND_STATUS_LENGTH_NOT_MULTIPLE\";\n",
        "\n",
        "\tcase CURAND_STATUS_DOUBLE_PRECISION_REQUIRED:\n",
        "\t\treturn \"CURAND_STATUS_DOUBLE_PRECISION_REQUIRED\";\n",
        "\n",
        "\tcase CURAND_STATUS_LAUNCH_FAILURE:\n",
        "\t\treturn \"CURAND_STATUS_LAUNCH_FAILURE\";\n",
        "\n",
        "\tcase CURAND_STATUS_PREEXISTING_FAILURE:\n",
        "\t\treturn \"CURAND_STATUS_PREEXISTING_FAILURE\";\n",
        "\n",
        "\tcase CURAND_STATUS_INITIALIZATION_FAILED:\n",
        "\t\treturn \"CURAND_STATUS_INITIALIZATION_FAILED\";\n",
        "\n",
        "\tcase CURAND_STATUS_ARCH_MISMATCH:\n",
        "\t\treturn \"CURAND_STATUS_ARCH_MISMATCH\";\n",
        "\n",
        "\tcase CURAND_STATUS_INTERNAL_ERROR:\n",
        "\t\treturn \"CURAND_STATUS_INTERNAL_ERROR\";\n",
        "\n",
        "\t}\n",
        "\n",
        "\treturn \"<unknown>\";\n",
        "}\n",
        "\n",
        "inline void __curandSafeCall(curandStatus_t err, const char *file, const int line)\n",
        "{\n",
        "\tif (CURAND_STATUS_SUCCESS != err) {\n",
        "\t\tfprintf(stderr, \"CURAND error in file '%s', line %d, error: %s \\nterminating!\\n\", __FILE__, __LINE__, \\\n",
        "\t\t\t_curandGetErrorEnum(err)); \\\n",
        "\t\t\tassert(0); \\\n",
        "\t}\n",
        "}\n",
        "\n",
        "extern \"C\" void curandSafeCall(curandStatus_t err) { __curandSafeCall(err, __FILE__, __LINE__); }\n",
        "\n",
        "/************************/\n",
        "/* REVERSE ARRAY KERNEL */\n",
        "/************************/\n",
        "#define BLOCKSIZE_REVERSE\t256\n",
        "\n",
        "// --- Credit to http://www.drdobbs.com/parallel/cuda-supercomputing-for-the-masses-part/208801731?pgno=2\n",
        "template <class T>\n",
        "__global__ void reverseArrayKernel(const T * __restrict__ d_in, T * __restrict__ d_out, const int N, const T a)\n",
        "{\n",
        "\t// --- Credit to the simpleTemplates CUDA sample\n",
        "\tSharedMemory<T> smem;\n",
        "\tT* s_data = smem.getPointer();\n",
        "\n",
        "\tconst int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "\tconst int id = threadIdx.x;\n",
        "\tconst int offset = blockDim.x * (blockIdx.x + 1);\n",
        "\n",
        "\t// --- Load one element per thread from device memory and store it *in reversed order* into shared memory\n",
        "\tif (tid < N) s_data[BLOCKSIZE_REVERSE - (id + 1)] = a * d_in[tid];\n",
        "\n",
        "\t// --- Block until all threads in the block have written their data to shared memory\n",
        "\t__syncthreads();\n",
        "\n",
        "\t// --- Write the data from shared memory in forward order\n",
        "\tif ((N - offset + id) >= 0) d_out[N - offset + id] = s_data[threadIdx.x];\n",
        "}\n",
        "\n",
        "/************************/\n",
        "/* REVERSE ARRAY KERNEL */\n",
        "/************************/\n",
        "template <class T>\n",
        "void reverseArray(const T * __restrict__ d_in, T * __restrict__ d_out, const int N, const T a) {\n",
        "\n",
        "\treverseArrayKernel << <iDivUp(N, BLOCKSIZE_REVERSE), BLOCKSIZE_REVERSE, BLOCKSIZE_REVERSE * sizeof(T) >> >(d_in, d_out, N, a);\n",
        "#ifdef DEBUG\n",
        "\tgpuErrchk(cudaPeekAtLastError());\n",
        "\tgpuErrchk(cudaDeviceSynchronize());\n",
        "#endif\n",
        "\n",
        "}\n",
        "\n",
        "template void reverseArray<float>(const float  * __restrict__, float  * __restrict__, const int, const float);\n",
        "template void reverseArray<double>(const double * __restrict__, double * __restrict__, const int, const double);\n",
        "\n",
        "/********************************************************/\n",
        "/* CARTESIAN TO POLAR COORDINATES TRANSFORMATION KERNEL */\n",
        "/********************************************************/\n",
        "#define BLOCKSIZE_CART2POL\t256\n",
        "\n",
        "template <class T>\n",
        "__global__ void Cartesian2PolarKernel(const T * __restrict__ d_x, const T * __restrict__ d_y, T * __restrict__ d_rho, T * __restrict__ d_theta,\n",
        "\tconst int N, const T a) {\n",
        "\n",
        "\tconst int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\tif (tid < N) {\n",
        "\t\td_rho[tid] = a * hypot(d_x[tid], d_y[tid]);\n",
        "\t\td_theta[tid] = atan2(d_y[tid], d_x[tid]);\n",
        "\t}\n",
        "\n",
        "}\n",
        "\n",
        "/*******************************************************/\n",
        "/* CARTESIAN TO POLAR COORDINATES TRANSFORMATION - GPU */\n",
        "/*******************************************************/\n",
        "//template <class T>\n",
        "//thrust::pair<T *,T *> Cartesian2Polar(const T * __restrict__ d_x, const T * __restrict__ d_y, const int N, const T a) {\n",
        "//\n",
        "//\tT *d_rho;\tgpuErrchk(cudaMalloc((void**)&d_rho,   N * sizeof(T)));\n",
        "//\tT *d_theta; gpuErrchk(cudaMalloc((void**)&d_theta, N * sizeof(T)));\n",
        "//\n",
        "//\tCartesian2PolarKernel<<<iDivUp(N, BLOCKSIZE_CART2POL), BLOCKSIZE_CART2POL>>>(d_x, d_y, d_rho, d_theta, N, a);\n",
        "//#ifdef DEBUG\n",
        "//\tgpuErrchk(cudaPeekAtLastError());\n",
        "//\tgpuErrchk(cudaDeviceSynchronize());\n",
        "//#endif\n",
        "//\n",
        "//\treturn thrust::make_pair(d_rho, d_theta);\n",
        "//}\n",
        "//\n",
        "//template thrust::pair<float  *, float  *>  Cartesian2Polar<float>  (const float  *, const float  *, const int, const float);\n",
        "//template thrust::pair<double *, double *>  Cartesian2Polar<double> (const double *, const double *, const int, const double);\n",
        "\n",
        "/*******************************************************/\n",
        "/* CARTESIAN TO POLAR COORDINATES TRANSFORMATION - CPU */\n",
        "/*******************************************************/\n",
        "//template <class T>\n",
        "//thrust::pair<T *,T *> h_Cartesian2Polar(const T * __restrict__ h_x, const T * __restrict__ h_y, const int N, const T a) {\n",
        "//\n",
        "//\tT *h_rho\t= (T *)malloc(N * sizeof(T));\n",
        "//\tT *h_theta\t= (T *)malloc(N * sizeof(T));\n",
        "//\n",
        "//\tfor (int i = 0; i < N; i++) {\n",
        "//\t\th_rho[i]\t= a * hypot(h_x[i], h_y[i]);\n",
        "//\t\th_theta[i]\t= atan2(h_y[i], h_x[i]);\n",
        "//\t}\n",
        "//\n",
        "//\treturn thrust::make_pair(h_rho, h_theta);\n",
        "//}\n",
        "//\n",
        "//template thrust::pair<float  *, float  *>  h_Cartesian2Polar<float>  (const float  *, const float  *, const int, const float);\n",
        "//template thrust::pair<double *, double *>  h_Cartesian2Polar<double> (const double *, const double *, const int, const double);\n",
        "\n",
        "/*******************************/\n",
        "/* COMPUTE L2 NORM OF A VECTOR */\n",
        "/*******************************/\n",
        "template<class T>\n",
        "T h_l2_norm(T *v1, T *v2, const int N) {\n",
        "\n",
        "\tT norm = (T)0;\n",
        "\n",
        "\tfor (int i = 0; i < N; ++i)\n",
        "\t{\n",
        "\t\tT d = v1[i] - v2[i];\n",
        "\t\tnorm = norm + d * d;\n",
        "\t}\n",
        "\n",
        "\treturn sqrt(norm);\n",
        "}\n",
        "\n",
        "template float  h_l2_norm<float>(float  *, float  *, const int);\n",
        "template double h_l2_norm<double>(double *, double *, const int);\n",
        "\n",
        "/*******************************/\n",
        "/* LINEAR COMBINATION FUNCTION */\n",
        "/*******************************/\n",
        "void linearCombination(const float * __restrict__ d_coeff, const float * __restrict__ d_basis_functions_real, float * __restrict__ d_linear_combination,\n",
        "\tconst int N_basis_functions, const int N_sampling_points, const cublasHandle_t handle) {\n",
        "\n",
        "\tfloat alpha = 1.f;\n",
        "\tfloat beta = 0.f;\n",
        "\tcublasSafeCall(cublasSgemv(handle, CUBLAS_OP_N, N_sampling_points, N_basis_functions, &alpha, d_basis_functions_real, N_sampling_points,\n",
        "\t\td_coeff, 1, &beta, d_linear_combination, 1));\n",
        "\n",
        "}\n",
        "\n",
        "void linearCombination(const double * __restrict__ d_coeff, const double * __restrict__ d_basis_functions_real, double * __restrict__ d_linear_combination,\n",
        "\tconst int N_basis_functions, const int N_sampling_points, const cublasHandle_t handle) {\n",
        "\n",
        "\tdouble alpha = 1.;\n",
        "\tdouble beta = 0.;\n",
        "\tcublasSafeCall(cublasDgemv(handle, CUBLAS_OP_N, N_sampling_points, N_basis_functions, &alpha, d_basis_functions_real, N_sampling_points,\n",
        "\t\td_coeff, 1, &beta, d_linear_combination, 1));\n",
        "\n",
        "}\n",
        "\n",
        "/******************************/\n",
        "/* ADD A CONSTANT TO A VECTOR */\n",
        "/******************************/\n",
        "#define BLOCKSIZE_VECTORADDCONSTANT\t256\n",
        "\n",
        "template<class T>\n",
        "__global__ void vectorAddConstantKernel(T * __restrict__ d_in, const T scalar, const int N) {\n",
        "\n",
        "\tconst int tid = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "\n",
        "\tif (tid < N) d_in[tid] += scalar;\n",
        "\n",
        "}\n",
        "\n",
        "template<class T>\n",
        "void vectorAddConstant(T * __restrict__ d_in, const T scalar, const int N) {\n",
        "\n",
        "\tvectorAddConstantKernel << <iDivUp(N, BLOCKSIZE_VECTORADDCONSTANT), BLOCKSIZE_VECTORADDCONSTANT >> >(d_in, scalar, N);\n",
        "\n",
        "}\n",
        "\n",
        "template void  vectorAddConstant<float>(float  * __restrict__, const float, const int);\n",
        "template void  vectorAddConstant<double>(double * __restrict__, const double, const int);\n",
        "\n",
        "/*****************************************/\n",
        "/* MULTIPLY A VECTOR BY A CONSTANT - GPU */\n",
        "/*****************************************/\n",
        "#define BLOCKSIZE_VECTORMULCONSTANT\t256\n",
        "\n",
        "template<class T>\n",
        "__global__ void vectorMulConstantKernel(T * __restrict__ d_in, const T scalar, const int N) {\n",
        "\n",
        "\tconst int tid = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "\n",
        "\tif (tid < N) d_in[tid] *= scalar;\n",
        "\n",
        "}\n",
        "\n",
        "template<class T>\n",
        "void vectorMulConstant(T * __restrict__ d_in, const T scalar, const int N) {\n",
        "\n",
        "\tvectorMulConstantKernel << <iDivUp(N, BLOCKSIZE_VECTORMULCONSTANT), BLOCKSIZE_VECTORMULCONSTANT >> >(d_in, scalar, N);\n",
        "\n",
        "}\n",
        "\n",
        "template void  vectorMulConstant<float>(float  * __restrict__, const float, const int);\n",
        "template void  vectorMulConstant<double>(double * __restrict__, const double, const int);\n",
        "\n",
        "/*****************************************/\n",
        "/* MULTIPLY A VECTOR BY A CONSTANT - CPU */\n",
        "/*****************************************/\n",
        "template<class T>\n",
        "void h_vectorMulConstant(T * __restrict__ h_in, const T scalar, const int N) {\n",
        "\n",
        "\tfor (int i = 0; i < N; i++) h_in[i] *= scalar;\n",
        "\n",
        "}\n",
        "\n",
        "template void  h_vectorMulConstant<float>(float  * __restrict__, const float, const int);\n",
        "template void  h_vectorMulConstant<double>(double * __restrict__, const double, const int);\n",
        "\n",
        "/*****************************************************/\n",
        "/* FUSED MULTIPLY ADD OPERATIONS FOR HOST AND DEVICE */\n",
        "/*****************************************************/\n",
        "template<class T>\n",
        "__host__ __device__ T fma2(T x, T y, T z) { return x * y + z; }\n",
        "\n",
        "template float  fma2<float >(float, float, float);\n",
        "template double fma2<double>(double, double, double);\n",
        "\n",
        "/*******************/\n",
        "/* MODULO FUNCTION */\n",
        "/*******************/\n",
        "__device__ int modulo(int val, int _mod)\n",
        "{\n",
        "\tint P;\n",
        "\tif (val > 0) { (!(_mod & (_mod - 1)) ? P = val&(_mod - 1) : P = val % (_mod)); return P; }\n",
        "\telse\n",
        "\t{\n",
        "\t\t(!(_mod & (_mod - 1)) ? P = (-val)&(_mod - 1) : P = (-val) % (_mod));\n",
        "\t\tif (P > 0) return _mod - P;\n",
        "\t\telse return 0;\n",
        "\t}\n",
        "}\n",
        "\n",
        "/***************************************/\n",
        "/* ATOMIC ADDITION FUNCTION ON DOUBLES */\n",
        "/***************************************/\n",
        "#if !defined(__CUDA_ARCH__) || __CUDA_ARCH__ >= 600\n",
        "#else\n",
        "__device__ double atomicAdd(double* address, double val)\n",
        "{\n",
        "\tunsigned long long int* address_as_ull =\n",
        "\t\t(unsigned long long int*)address;\n",
        "\tregister unsigned long long int old = *address_as_ull, assumed;\n",
        "\tdo {\n",
        "\t\tassumed = old;\n",
        "\t\told = atomicCAS(address_as_ull, assumed,\n",
        "\t\t\t__double_as_longlong(val +\n",
        "\t\t\t\t__longlong_as_double(assumed)));\n",
        "\t} while (assumed != old);\n",
        "\treturn __longlong_as_double(old);\n",
        "}\n",
        "#endif\n",
        "\n",
        "/*********************************/\n",
        "/* ATOMIC MIN FUNCTION ON FLOATS */\n",
        "/*********************************/\n",
        "__device__ float atomicMin(float* address, float val)\n",
        "{\n",
        "\tint* address_as_i = (int*)address;\n",
        "\tint old = *address_as_i, assumed;\n",
        "\tdo {\n",
        "\t\tassumed = old;\n",
        "\t\told = ::atomicCAS(address_as_i, assumed,\n",
        "\t\t\t__float_as_int(::fminf(val, __int_as_float(assumed))));\n",
        "\t} while (assumed != old);\n",
        "\treturn __int_as_float(old);\n",
        "}\n",
        "\n",
        "/*********************/\n",
        "/* DEGREE TO RADIANS */\n",
        "/*********************/\n",
        "double deg2rad(double deg) { return deg*PI_R / 180; }\n",
        "\n",
        "/*********************/\n",
        "/* CUDA MEMORY USAGE */\n",
        "/*********************/\n",
        "void cudaMemoryUsage() {\n",
        "\n",
        "\tsize_t free_byte;\n",
        "\tsize_t total_byte;\n",
        "\n",
        "\tgpuErrchk(cudaMemGetInfo(&free_byte, &total_byte));\n",
        "\n",
        "\tdouble free_db = (double)free_byte;\n",
        "\tdouble total_db = (double)total_byte;\n",
        "\tdouble used_db = total_db - free_db;\n",
        "\n",
        "\tprintf(\"GPU memory: used = %f, free = %f MB, total available = %f MB\\n\", used_db / 1024.0 / 1024.0, free_db / 1024.0 / 1024.0, total_db / 1024.0 / 1024.0);\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mY_fWZB3n6qC",
        "outputId": "71e84c24-1c4b-4270-c1b1-c83120c55a46"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/Utilities.cu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda --name TimingGPU.cu\n",
        "\n",
        "/**************/\n",
        "/* TIMING GPU */\n",
        "/**************/\n",
        "\n",
        "#include \"TimingGPU.h\"\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "struct PrivateTimingGPU {\n",
        "\tcudaEvent_t\t\tstart;\n",
        "\tcudaEvent_t\t\tstop;\n",
        "};\n",
        "\n",
        "// default constructor\n",
        "TimingGPU::TimingGPU() { privateTimingGPU = new PrivateTimingGPU; }\n",
        "\n",
        "// default destructor\n",
        "TimingGPU::~TimingGPU() { }\n",
        "\n",
        "void TimingGPU::StartCounter()\n",
        "{\n",
        "\tcudaEventCreate(&((*privateTimingGPU).start));\n",
        "\tcudaEventCreate(&((*privateTimingGPU).stop));\n",
        "\tcudaEventRecord((*privateTimingGPU).start, 0);\n",
        "}\n",
        "\n",
        "void TimingGPU::StartCounterFlags()\n",
        "{\n",
        "\tint eventflags = cudaEventBlockingSync;\n",
        "\n",
        "\tcudaEventCreateWithFlags(&((*privateTimingGPU).start), eventflags);\n",
        "\tcudaEventCreateWithFlags(&((*privateTimingGPU).stop), eventflags);\n",
        "\tcudaEventRecord((*privateTimingGPU).start, 0);\n",
        "}\n",
        "\n",
        "// Gets the counter in ms\n",
        "float TimingGPU::GetCounter()\n",
        "{\n",
        "\tfloat\ttime;\n",
        "\tcudaEventRecord((*privateTimingGPU).stop, 0);\n",
        "\tcudaEventSynchronize((*privateTimingGPU).stop);\n",
        "\tcudaEventElapsedTime(&time, (*privateTimingGPU).start, (*privateTimingGPU).stop);\n",
        "\treturn time;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NUPp1QX9nMBO",
        "outputId": "312248d3-223b-445b-e6f1-2dff62860ad4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/TimingGPU.cu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda --name FFTShift_1D.cu\n",
        "\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#include <cufft.h>\n",
        "#include <cufftXt.h>\n",
        "\n",
        "#include \"TimingGPU.h\"\n",
        "#include \"Utilities.h\"\n",
        "\n",
        "//#define DEBUG\n",
        "\n",
        "#define BLOCKSIZE 256\n",
        "\n",
        "/*****************************************/\n",
        "/* FFTSHIFT 1D IN-PLACE MEMORY MOVEMENTS */\n",
        "/*****************************************/\n",
        "__global__ void fftshift_1D_inplace_memory_movements(float2 * __restrict__ d_inout, const unsigned int N)\n",
        "{\n",
        "\tunsigned int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "\tif (tid < N/2)\n",
        "    {\n",
        "\t\tfloat2 temp = d_inout[tid];\n",
        "        d_inout[tid] = d_inout[tid + (N / 2)];\n",
        "        d_inout[tid + (N / 2)] = temp;\n",
        "    }\n",
        "}\n",
        "\n",
        "/*********************************************/\n",
        "/* FFTSHIFT 1D OUT-OF-PLACE MEMORY MOVEMENTS */\n",
        "/*********************************************/\n",
        "__global__ void fftshift_1D_outofplace_memory_movements(const float2 * __restrict__ d_in, float2 * __restrict__ d_out, const unsigned int N)\n",
        "{\n",
        "\tunsigned int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "\tif (tid < N/2)\n",
        "    {\n",
        "        d_out[tid] = d_in[tid + (N / 2)];\n",
        "        d_out[tid + (N / 2)] = d_in[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "/**********************************************/\n",
        "/* FFTSHIFT 1D INPLACE CHESSBOARD - VERSION 1 */\n",
        "/**********************************************/\n",
        "static __device__ float2 fftshift_1D_chessboard_callback_v1(void *d_in, size_t offset, void *callerInfo, void *sharedPtr) {\n",
        "\n",
        "\tfloat a\t\t= (float)(1-2*((int)offset%2));\n",
        "\n",
        "\tfloat2\tout = ((float2*)d_in)[offset];\n",
        "\tout.x = out.x * a;\n",
        "\tout.y = out.y * a;\n",
        "\treturn out;\n",
        "}\n",
        "\n",
        "__device__ cufftCallbackLoadC fftshift_1D_chessboard_callback_v1_Ptr = fftshift_1D_chessboard_callback_v1;\n",
        "\n",
        "/**********************************************/\n",
        "/* FFTSHIFT 1D INPLACE CHESSBOARD - VERSION 2 */\n",
        "/**********************************************/\n",
        "__device__ float2 fftshift_1D_chessboard_callback_v2(void *d_in, size_t offset, void *callerInfo, void *sharedPtr) {\n",
        "\n",
        "\tfloat a = pow(-1, (offset&1));\n",
        "\n",
        "\tfloat2\tout = ((float2*)d_in)[offset];\n",
        "\tout.x = out.x * a;\n",
        "\tout.y = out.y * a;\n",
        "\treturn out;\n",
        "}\n",
        "\n",
        "__device__ cufftCallbackLoadC fftshift_1D_chessboard_callback_v2_Ptr = fftshift_1D_chessboard_callback_v2;\n",
        "\n",
        "/**********************************************/\n",
        "/* FFTSHIFT 1D INPLACE CHESSBOARD - VERSION 3 */\n",
        "/**********************************************/\n",
        "__device__ float2 fftshift_1D_chessboard_callback_v3(void *d_in, size_t offset, void *callerInfo, void *sharedPtr) {\n",
        "\n",
        "\tfloat2\tout = ((float2*)d_in)[offset];\n",
        "\n",
        "\tif ((int)offset&1) {\n",
        "\n",
        "\t\tout.x = -out.x;\n",
        "\t\tout.y = -out.y;\n",
        "\n",
        "\t}\n",
        "\treturn out;\n",
        "}\n",
        "\n",
        "__device__ cufftCallbackLoadC fftshift_1D_chessboard_callback_v3_Ptr = fftshift_1D_chessboard_callback_v3;\n",
        "\n",
        "/********/\n",
        "/* MAIN */\n",
        "/********/\n",
        "int main()\n",
        "{\n",
        "\t//const int N = 131072;\n",
        "\t//const int N = 524288;\n",
        "\tconst int N = 2097152;\n",
        "  //const int N = 64;\n",
        "\n",
        "\tTimingGPU timerGPU;\n",
        "\n",
        "\t// --- Host side input array\n",
        "\tfloat2 *h_vect = (float2 *)malloc(N * sizeof(float2));\n",
        "\tfor (int i = 0; i < N; i++) {\n",
        "\t\th_vect[i].x = (float)rand() / (float)RAND_MAX;\n",
        "\t\th_vect[i].y = (float)rand() / (float)RAND_MAX;\n",
        "\t}\n",
        "\n",
        "\t// --- Host side output arrays\n",
        "\tfloat2 *h_out1 = (float2 *)malloc(N * sizeof(float2));\n",
        "\tfloat2 *h_out2 = (float2 *)malloc(N * sizeof(float2));\n",
        "\tfloat2 *h_out3 = (float2 *)malloc(N * sizeof(float2));\n",
        "\tfloat2 *h_out4 = (float2 *)malloc(N * sizeof(float2));\n",
        "\tfloat2 *h_out5 = (float2 *)malloc(N * sizeof(float2));\n",
        "\n",
        "\t// --- Device side input arrays\n",
        "\tfloat2 *d_vect1; gpuErrchk(cudaMalloc(&d_vect1, N * sizeof(float2)));\n",
        "\tfloat2 *d_vect2; gpuErrchk(cudaMalloc(&d_vect2, N * sizeof(float2)));\n",
        "\tfloat2 *d_vect3; gpuErrchk(cudaMalloc(&d_vect3, N * sizeof(float2)));\n",
        "\tfloat2 *d_vect4; gpuErrchk(cudaMalloc(&d_vect4, N * sizeof(float2)));\n",
        "\tfloat2 *d_vect5; gpuErrchk(cudaMalloc(&d_vect5, N * sizeof(float2)));\n",
        " \tgpuErrchk(cudaMemcpy(d_vect1, h_vect, N * sizeof(float2), cudaMemcpyHostToDevice));\n",
        "\tgpuErrchk(cudaMemcpy(d_vect2, h_vect, N * sizeof(float2), cudaMemcpyHostToDevice));\n",
        "\tgpuErrchk(cudaMemcpy(d_vect3, h_vect, N * sizeof(float2), cudaMemcpyHostToDevice));\n",
        "\tgpuErrchk(cudaMemcpy(d_vect4, h_vect, N * sizeof(float2), cudaMemcpyHostToDevice));\n",
        "\tgpuErrchk(cudaMemcpy(d_vect5, h_vect, N * sizeof(float2), cudaMemcpyHostToDevice));\n",
        "\n",
        "\t// --- Device side output arrays\n",
        "\tfloat2 *d_out1; gpuErrchk(cudaMalloc(&d_out1, N * sizeof(float2)));\n",
        "\tfloat2 *d_out2; gpuErrchk(cudaMalloc(&d_out2, N * sizeof(float2)));\n",
        "\tfloat2 *d_out3; gpuErrchk(cudaMalloc(&d_out3, N * sizeof(float2)));\n",
        "\tfloat2 *d_out4; gpuErrchk(cudaMalloc(&d_out4, N * sizeof(float2)));\n",
        "\tfloat2 *d_out5; gpuErrchk(cudaMalloc(&d_out5, N * sizeof(float2)));\n",
        "\n",
        "\t/***************************************************************/\n",
        "\t/* VERSION 1: cuFFT + IN-PLACE MEMORY MOVEMENTS BASED FFTSHIFT */\n",
        "\t/***************************************************************/\n",
        "\tcufftHandle planinverse; cufftSafeCall(cufftPlan1d(&planinverse, N, CUFFT_C2C, 1));\n",
        "\ttimerGPU.StartCounter();\n",
        "\tcufftSafeCall(cufftExecC2C(planinverse, d_vect1, d_vect1, CUFFT_INVERSE));\n",
        "\tfftshift_1D_inplace_memory_movements<<<iDivUp(N/2, BLOCKSIZE), BLOCKSIZE>>>(d_vect1, N);\n",
        "#ifdef DEBUG\n",
        "\tgpuErrchk(cudaPeekAtLastError());\n",
        "\tgpuErrchk(cudaDeviceSynchronize());\n",
        "#endif\n",
        "\tprintf(\"In-place memory movements elapsed time:  %3.3f ms \\n\", timerGPU.GetCounter());\n",
        "\tgpuErrchk(cudaMemcpy(h_out1, d_vect1, N * sizeof(float2), cudaMemcpyDeviceToHost));\n",
        "\n",
        "\t/*******************************************************************/\n",
        "\t/* VERSION 2: cuFFT + OUT-OF-PLACE MEMORY MOVEMENTS BASED FFTSHIFT */\n",
        "\t/*******************************************************************/\n",
        "\tcufftHandle planinverse_v1; cufftSafeCall(cufftPlan1d(&planinverse_v1, N, CUFFT_C2C, 1));\n",
        "\ttimerGPU.StartCounter();\n",
        "\tcufftSafeCall(cufftExecC2C(planinverse_v1, d_vect2, d_vect2, CUFFT_INVERSE));\n",
        "\tfftshift_1D_outofplace_memory_movements<<<iDivUp(N/2, BLOCKSIZE), BLOCKSIZE>>>(d_vect2, d_out2, N);\n",
        "#ifdef DEBUG\n",
        "\tgpuErrchk(cudaPeekAtLastError());\n",
        "\tgpuErrchk(cudaDeviceSynchronize());\n",
        "#endif\n",
        "\tprintf(\"Out-of-place memory movements elapsed time:  %3.3f ms \\n\", timerGPU.GetCounter());\n",
        "\tgpuErrchk(cudaMemcpy(h_out2, d_out2, N * sizeof(float2), cudaMemcpyDeviceToHost));\n",
        "\n",
        "\t// --- Checking the results\n",
        "\tfor (int i=0; i<N; i++) if ((h_out1[i].x != h_out2[i].x)||(h_out1[i].y != h_out2[i].y)) { printf(\"Out-of-place memory movements test failed!\\n\"); return 0; }\n",
        "\n",
        "\tprintf(\"Out-of-place memory movements test passed!\\n\");\n",
        "\n",
        "\t/***************************************************/\n",
        "\t/* VERSION 3: CHESSBOARD MULTIPLICATION V1 + cuFFT */\n",
        "\t/***************************************************/\n",
        "\tcufftCallbackLoadC hfftshift_1D_chessboard_callback_v1_Ptr;\n",
        "\n",
        "\tgpuErrchk(cudaMemcpyFromSymbol(&hfftshift_1D_chessboard_callback_v1_Ptr, fftshift_1D_chessboard_callback_v1_Ptr, sizeof(hfftshift_1D_chessboard_callback_v1_Ptr)));\n",
        "\tcufftHandle planinverse_v2; cufftSafeCall(cufftCreate(&planinverse_v2));\n",
        "\tsize_t work_size_v2; cufftSafeCall(cufftMakePlan1d(planinverse_v2, N, CUFFT_C2C, 1, &work_size_v2));\n",
        "\tcufftSafeCall(cufftXtSetCallback(planinverse_v2, (void **)&hfftshift_1D_chessboard_callback_v1_Ptr, CUFFT_CB_LD_COMPLEX, NULL));\n",
        "\ttimerGPU.StartCounter();\n",
        "\tcufftSafeCall(cufftExecC2C(planinverse_v2, d_vect3, d_out3, CUFFT_INVERSE));\n",
        "\tprintf(\"Chessboard v1 elapsed time:  %3.3f ms \\n\", timerGPU.GetCounter());\n",
        "\n",
        "\tgpuErrchk(cudaMemcpy(h_out3, d_out3, N*sizeof(float2), cudaMemcpyDeviceToHost));\n",
        "\n",
        "\t// --- Checking the results\n",
        "\tfor (int i=0; i<N; i++) if ((h_out1[i].x != h_out3[i].x)||(h_out1[i].y != h_out3[i].y)) { printf(\"Chessboard v1 test failed!\\n\"); return 0; }\n",
        "\n",
        "\tprintf(\"Chessboard v1 test passed!\\n\");\n",
        "\n",
        "\t/****************************************/\n",
        "\t/* CHESSBOARD MULTIPLICATION V2 + cuFFT */\n",
        "\t/****************************************/\n",
        "\tcufftCallbackLoadC hfftshift_1D_chessboard_callback_v2_Ptr;\n",
        "\n",
        "\tgpuErrchk(cudaMemcpyFromSymbol(&hfftshift_1D_chessboard_callback_v2_Ptr, fftshift_1D_chessboard_callback_v2_Ptr, sizeof(hfftshift_1D_chessboard_callback_v2_Ptr)));\n",
        "\tcufftHandle planinverse_v3; cufftSafeCall(cufftCreate(&planinverse_v3));\n",
        "\tsize_t work_size_v3; cufftSafeCall(cufftMakePlan1d(planinverse_v3, N, CUFFT_C2C, 1, &work_size_v3));\n",
        "\tcufftSafeCall(cufftXtSetCallback(planinverse_v3, (void **)&hfftshift_1D_chessboard_callback_v2_Ptr, CUFFT_CB_LD_COMPLEX, 0));\n",
        "\ttimerGPU.StartCounter();\n",
        "\tcufftSafeCall(cufftExecC2C(planinverse_v3, (float2 *)d_vect4, (float2 *)d_out4, CUFFT_INVERSE));\n",
        "\tprintf(\"Chessboard v2 elapsed time:  %3.3f ms \\n\", timerGPU.GetCounter());\n",
        "\n",
        "\tgpuErrchk(cudaMemcpy(h_out4, d_out4, N*sizeof(float2), cudaMemcpyDeviceToHost));\n",
        "\n",
        "\t// --- Checking the results\n",
        "\tfor (int i=0; i<N; i++) if ((h_out1[i].x != h_out4[i].x)||(h_out1[i].y != h_out4[i].y)) { printf(\"Chessboard v2 test failed!\\n\"); return 0; }\n",
        "\n",
        "\tprintf(\"Chessboard v2 test passed!\\n\");\n",
        "\n",
        "\t/****************************************/\n",
        "\t/* CHESSBOARD MULTIPLICATION V3 + cuFFT */\n",
        "\t/****************************************/\n",
        "\tcufftCallbackLoadC hfftshift_1D_chessboard_callback_v3_Ptr;\n",
        "\n",
        "\tgpuErrchk(cudaMemcpyFromSymbol(&hfftshift_1D_chessboard_callback_v3_Ptr, fftshift_1D_chessboard_callback_v3_Ptr, sizeof(hfftshift_1D_chessboard_callback_v3_Ptr)));\n",
        "\tcufftHandle planinverse_v4; cufftSafeCall(cufftCreate(&planinverse_v4));\n",
        "\tsize_t work_size_v4; cufftSafeCall(cufftMakePlan1d(planinverse_v4, N, CUFFT_C2C, 1, &work_size_v4));\n",
        "\tcufftSafeCall(cufftXtSetCallback(planinverse_v4, (void **)&hfftshift_1D_chessboard_callback_v3_Ptr, CUFFT_CB_LD_COMPLEX, 0));\n",
        "\ttimerGPU.StartCounter();\n",
        "\tcufftSafeCall(cufftExecC2C(planinverse_v4, d_vect5, d_out5, CUFFT_INVERSE));\n",
        "\tprintf(\"Chessboard v3 elapsed time:  %3.3f ms \\n\", timerGPU.GetCounter());\n",
        "\n",
        "\tgpuErrchk(cudaMemcpy(h_out5, d_out5, N*sizeof(float2), cudaMemcpyDeviceToHost));\n",
        "\n",
        "\t// --- Checking the results\n",
        "\tfor (int i=0; i<N; i++) if ((h_out1[i].x != h_out5[i].x)||(h_out1[i].y != h_out5[i].y)) { printf(\"Chessboard v3 test failed!\\n\"); return 0; }\n",
        "\n",
        "\tprintf(\"Chessboard v3 test passed!\\n\");\n",
        "\n",
        "\treturn 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lbGZrJ1_vXGf",
        "outputId": "dd5ae6ff-5876-4452-bcf4-ade4291cba54"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/FFTShift_1D.cu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o \"/content/src/FFTShift_1D.o\" /content/src/FFTShift_1D.cu /content/src/TimingGPU.cu /content/src/Utilities.cu -lcufft_static -lculibos -lcublas --relocatable-device-code=true"
      ],
      "metadata": {
        "id": "KxSdPAvkwNuN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 755 /content/src/FFTShift_1D.o\n",
        "!/content/src/FFTShift_1D.o"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cds1kNjvouZU",
        "outputId": "107c619b-6a85-4ce1-b2b0-f6b47026c9c1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-place memory movements elapsed time:  0.713 ms \n",
            "Out-of-place memory movements elapsed time:  0.715 ms \n",
            "Out-of-place memory movements test passed!\n",
            "Chessboard v1 elapsed time:  0.610 ms \n",
            "Chessboard v1 test passed!\n",
            "Chessboard v2 elapsed time:  5.357 ms \n",
            "Chessboard v2 test passed!\n",
            "Chessboard v3 elapsed time:  0.605 ms \n",
            "Chessboard v3 test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/zchee/cuda-sample/blob/master/7_CUDALibraries/simpleCUFFT_callback/simpleCUFFT_callback.cu"
      ],
      "metadata": {
        "id": "Gf1Y1mFal5hg"
      }
    }
  ]
}
